{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82ca4b04",
   "metadata": {},
   "source": [
    "# Q2: Text-Driven Image Segmentation with SAM 2\n",
    "\n",
    "Implementation of text-prompted image segmentation using SAM 2 (Segment Anything Model 2) with GroundingDINO for text-to-region conversion.\n",
    "\n",
    "## Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17d2210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install torch torchvision torchaudio\n",
    "!pip install transformers\n",
    "!pip install opencv-python\n",
    "!pip install matplotlib\n",
    "!pip install pillow\n",
    "!pip install numpy\n",
    "!pip install requests\n",
    "\n",
    "# Install SAM 2\n",
    "!pip install git+https://github.com/facebookresearch/segment-anything-2.git\n",
    "\n",
    "# Install GroundingDINO\n",
    "!pip install git+https://github.com/IDEA-Research/GroundingDINO.git\n",
    "\n",
    "# Alternative: Install supervision for additional utilities\n",
    "!pip install supervision\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02588df",
   "metadata": {},
   "source": [
    "## Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b8c3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SAM 2\n",
    "try:\n",
    "    from sam2.build_sam import build_sam2\n",
    "    from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
    "    \n",
    "    # Download SAM 2 checkpoint\n",
    "    checkpoint = \"./sam2_hiera_large.pt\"\n",
    "    model_cfg = \"sam2_hiera_l.yaml\"\n",
    "    \n",
    "    # Try to download if not exists\n",
    "    import os\n",
    "    if not os.path.exists(checkpoint):\n",
    "        !wget https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_large.pt\n",
    "    \n",
    "    sam2_model = build_sam2(model_cfg, checkpoint, device=device)\n",
    "    predictor = SAM2ImagePredictor(sam2_model)\n",
    "    print(\"SAM 2 loaded successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading SAM 2: {e}\")\n",
    "    print(\"Falling back to alternative implementation...\")\n",
    "    \n",
    "    # Alternative: Use transformers SAM model\n",
    "    from transformers import SamModel, SamProcessor\n",
    "    \n",
    "    model_name = \"facebook/sam-vit-huge\"\n",
    "    sam_model = SamModel.from_pretrained(model_name).to(device)\n",
    "    sam_processor = SamProcessor.from_pretrained(model_name)\n",
    "    print(\"Alternative SAM model loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9653e9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GroundingDINO for text-to-bbox conversion\n",
    "try:\n",
    "    from groundingdino.models import build_model\n",
    "    from groundingdino.util.slconfig import SLConfig\n",
    "    from groundingdino.util.utils import clean_state_dict, get_phrases_from_posmap\n",
    "    from groundingdino.util.inference import annotate, load_model, predict\n",
    "    \n",
    "    # Load GroundingDINO\n",
    "    grounding_model = load_model(\"groundingdino/config/GroundingDINO_SwinT_OGC.py\", \n",
    "                                \"weights/groundingdino_swint_ogc.pth\")\n",
    "    print(\"GroundingDINO loaded successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading GroundingDINO: {e}\")\n",
    "    print(\"Will use alternative text-to-region method...\")\n",
    "    grounding_model = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb570b8",
   "metadata": {},
   "source": [
    "## Alternative Text-to-Region Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21792031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative implementation using CLIP for text-to-region\n",
    "!pip install clip-by-openai\n",
    "\n",
    "import clip\n",
    "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize\n",
    "\n",
    "# Load CLIP model\n",
    "clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "print(\"CLIP model loaded for text understanding!\")\n",
    "\n",
    "def sliding_window_clip(image, text_prompt, window_size=224, stride=112, threshold=0.25):\n",
    "    \"\"\"Use sliding window with CLIP to find regions matching text prompt.\"\"\"\n",
    "    h, w = image.shape[:2]\n",
    "    best_boxes = []\n",
    "    best_scores = []\n",
    "    \n",
    "    # Convert to PIL for CLIP preprocessing\n",
    "    pil_image = Image.fromarray(image)\n",
    "    \n",
    "    # Tokenize text\n",
    "    text_tokens = clip.tokenize([text_prompt]).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        text_features = clip_model.encode_text(text_tokens)\n",
    "        text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "    \n",
    "    # Sliding window\n",
    "    for y in range(0, h - window_size + 1, stride):\n",
    "        for x in range(0, w - window_size + 1, stride):\n",
    "            # Extract window\n",
    "            window = pil_image.crop((x, y, x + window_size, y + window_size))\n",
    "            \n",
    "            # Preprocess for CLIP\n",
    "            window_tensor = clip_preprocess(window).unsqueeze(0).to(device)\n",
    "            \n",
    "            # Get image features\n",
    "            with torch.no_grad():\n",
    "                image_features = clip_model.encode_image(window_tensor)\n",
    "                image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "                \n",
    "                # Calculate similarity\n",
    "                similarity = (image_features @ text_features.T).item()\n",
    "            \n",
    "            if similarity > threshold:\n",
    "                best_boxes.append([x, y, x + window_size, y + window_size])\n",
    "                best_scores.append(similarity)\n",
    "    \n",
    "    # Non-maximum suppression (simple version)\n",
    "    if best_boxes:\n",
    "        # Sort by score\n",
    "        sorted_indices = sorted(range(len(best_scores)), key=lambda i: best_scores[i], reverse=True)\n",
    "        \n",
    "        # Take top box\n",
    "        best_idx = sorted_indices[0]\n",
    "        return [best_boxes[best_idx]], [best_scores[best_idx]]\n",
    "    \n",
    "    return [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f8c6f0",
   "metadata": {},
   "source": [
    "## Text-to-Segmentation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea76ef58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_segmentation_pipeline(image_path, text_prompt, use_url=False):\n",
    "    \"\"\"Complete pipeline from text prompt to segmentation mask.\"\"\"\n",
    "    \n",
    "    # Load image\n",
    "    if use_url:\n",
    "        response = requests.get(image_path)\n",
    "        image = Image.open(BytesIO(response.content)).convert('RGB')\n",
    "        image_np = np.array(image)\n",
    "    else:\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        image_np = np.array(image)\n",
    "    \n",
    "    print(f\"Image loaded: {image_np.shape}\")\n",
    "    print(f\"Text prompt: '{text_prompt}'\")\n",
    "    \n",
    "    # Step 1: Text to bounding boxes\n",
    "    if grounding_model is not None:\n",
    "        # Use GroundingDINO if available\n",
    "        try:\n",
    "            boxes, logits, phrases = predict(\n",
    "                model=grounding_model, \n",
    "                image=image_np, \n",
    "                caption=text_prompt,\n",
    "                box_threshold=0.3,\n",
    "                text_threshold=0.25\n",
    "            )\n",
    "            \n",
    "            # Convert to pixel coordinates\n",
    "            h, w = image_np.shape[:2]\n",
    "            boxes_pixel = boxes * torch.tensor([w, h, w, h])\n",
    "            boxes_pixel = boxes_pixel.cpu().numpy()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"GroundingDINO failed: {e}, using CLIP fallback\")\n",
    "            boxes_pixel, scores = sliding_window_clip(image_np, text_prompt)\n",
    "            boxes_pixel = np.array(boxes_pixel) if boxes_pixel else np.array([])\n",
    "    else:\n",
    "        # Use CLIP sliding window\n",
    "        boxes_pixel, scores = sliding_window_clip(image_np, text_prompt)\n",
    "        boxes_pixel = np.array(boxes_pixel) if boxes_pixel else np.array([])\n",
    "    \n",
    "    if len(boxes_pixel) == 0:\n",
    "        print(\"No regions found for the text prompt. Using center point.\")\n",
    "        h, w = image_np.shape[:2]\n",
    "        # Use center as fallback\n",
    "        input_points = np.array([[w//2, h//2]])\n",
    "        input_labels = np.array([1])\n",
    "    else:\n",
    "        # Convert boxes to points (center of first box)\n",
    "        box = boxes_pixel[0]\n",
    "        center_x = (box[0] + box[2]) / 2\n",
    "        center_y = (box[1] + box[3]) / 2\n",
    "        input_points = np.array([[center_x, center_y]])\n",
    "        input_labels = np.array([1])\n",
    "        \n",
    "        print(f\"Found {len(boxes_pixel)} regions, using center point: ({center_x:.1f}, {center_y:.1f})\")\n",
    "    \n",
    "    # Step 2: SAM segmentation\n",
    "    try:\n",
    "        # Try SAM 2 first\n",
    "        predictor.set_image(image_np)\n",
    "        masks, scores, logits = predictor.predict(\n",
    "            point_coords=input_points,\n",
    "            point_labels=input_labels,\n",
    "            multimask_output=True,\n",
    "        )\n",
    "        \n",
    "        # Select best mask\n",
    "        best_mask_idx = np.argmax(scores)\n",
    "        best_mask = masks[best_mask_idx]\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"SAM 2 failed: {e}, using alternative SAM\")\n",
    "        \n",
    "        # Use transformers SAM\n",
    "        inputs = sam_processor(image, input_points=[input_points.tolist()], return_tensors=\"pt\").to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = sam_model(**inputs)\n",
    "        \n",
    "        masks = sam_processor.image_processor.post_process_masks(\n",
    "            outputs.pred_masks.cpu(), inputs[\"original_sizes\"].cpu(), inputs[\"reshaped_input_sizes\"].cpu()\n",
    "        )\n",
    "        best_mask = masks[0][0][0].numpy()\n",
    "    \n",
    "    return image_np, best_mask, input_points\n",
    "\n",
    "def visualize_segmentation(image, mask, points, text_prompt):\n",
    "    \"\"\"Visualize the segmentation result.\"\"\"\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Original image\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(image)\n",
    "    plt.title('Original Image')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Mask\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(mask, cmap='gray')\n",
    "    plt.title('Segmentation Mask')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Overlay\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(image)\n",
    "    \n",
    "    # Create colored mask overlay\n",
    "    colored_mask = np.zeros_like(image)\n",
    "    colored_mask[:, :, 0] = mask * 255  # Red channel\n",
    "    \n",
    "    # Blend with original image\n",
    "    overlay = image.copy()\n",
    "    overlay[mask > 0] = overlay[mask > 0] * 0.6 + colored_mask[mask > 0] * 0.4\n",
    "    \n",
    "    plt.imshow(overlay.astype(np.uint8))\n",
    "    \n",
    "    # Plot input points\n",
    "    for point in points:\n",
    "        plt.plot(point[0], point[1], 'go', markersize=10, markeredgewidth=2, markeredgecolor='white')\n",
    "    \n",
    "    plt.title(f'Segmentation: \"{text_prompt}\"')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27354a5",
   "metadata": {},
   "source": [
    "## Example 1: Segment a Dog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a1ccb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download a sample image\n",
    "image_url = \"https://images.unsplash.com/photo-1552053831-71594a27632d?ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&auto=format&fit=crop&w=1000&q=80\"\n",
    "text_prompt = \"dog\"\n",
    "\n",
    "# Run the pipeline\n",
    "image, mask, points = text_to_segmentation_pipeline(image_url, text_prompt, use_url=True)\n",
    "\n",
    "# Visualize results\n",
    "visualize_segmentation(image, mask, points, text_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4e4fa5",
   "metadata": {},
   "source": [
    "## Example 2: Segment a Car"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3c1a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another example with a car\n",
    "image_url = \"https://images.unsplash.com/photo-1549317661-bd32c8ce0db2?ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&auto=format&fit=crop&w=1000&q=80\"\n",
    "text_prompt = \"car\"\n",
    "\n",
    "# Run the pipeline\n",
    "image, mask, points = text_to_segmentation_pipeline(image_url, text_prompt, use_url=True)\n",
    "\n",
    "# Visualize results\n",
    "visualize_segmentation(image, mask, points, text_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0cf0167",
   "metadata": {},
   "source": [
    "## Example 3: Custom Image Upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59adddce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive example - you can upload your own image\n",
    "from google.colab import files\n",
    "import os\n",
    "\n",
    "print(\"Upload an image file:\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Get the uploaded filename\n",
    "filename = list(uploaded.keys())[0]\n",
    "print(f\"Uploaded: {filename}\")\n",
    "\n",
    "# Text prompt input\n",
    "text_prompt = input(\"Enter text prompt for segmentation (e.g., 'person', 'cat', 'tree'): \")\n",
    "\n",
    "# Run the pipeline\n",
    "image, mask, points = text_to_segmentation_pipeline(filename, text_prompt, use_url=False)\n",
    "\n",
    "# Visualize results\n",
    "visualize_segmentation(image, mask, points, text_prompt)\n",
    "\n",
    "# Clean up\n",
    "os.remove(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff4fb0c",
   "metadata": {},
   "source": [
    "## Bonus: Video Segmentation with SAM 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84121bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Video segmentation implementation\n",
    "try:\n",
    "    from sam2.build_sam import build_sam2_video_predictor\n",
    "    \n",
    "    # Build video predictor\n",
    "    video_predictor = build_sam2_video_predictor(\"sam2_hiera_l.yaml\", \"./sam2_hiera_large.pt\")\n",
    "    \n",
    "    def video_segmentation_pipeline(video_path, text_prompt, frame_interval=5):\n",
    "        \"\"\"Segment object in video using text prompt.\"\"\"\n",
    "        \n",
    "        # Extract frames from video\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        frames = []\n",
    "        frame_count = 0\n",
    "        \n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            \n",
    "            if frame_count % frame_interval == 0:\n",
    "                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                frames.append(frame_rgb)\n",
    "            \n",
    "            frame_count += 1\n",
    "            \n",
    "            # Limit to 30 frames for demo\n",
    "            if len(frames) >= 30:\n",
    "                break\n",
    "        \n",
    "        cap.release()\n",
    "        \n",
    "        if not frames:\n",
    "            print(\"No frames extracted from video\")\n",
    "            return\n",
    "        \n",
    "        print(f\"Extracted {len(frames)} frames\")\n",
    "        \n",
    "        # Initialize video predictor\n",
    "        inference_state = video_predictor.init_state(video_path=None, frames=frames)\n",
    "        \n",
    "        # Get initial segmentation from first frame\n",
    "        first_frame = frames[0]\n",
    "        \n",
    "        # Use text-to-region on first frame\n",
    "        boxes_pixel, scores = sliding_window_clip(first_frame, text_prompt)\n",
    "        \n",
    "        if boxes_pixel:\n",
    "            box = boxes_pixel[0]\n",
    "            center_x = (box[0] + box[2]) / 2\n",
    "            center_y = (box[1] + box[3]) / 2\n",
    "            input_points = np.array([[center_x, center_y]])\n",
    "            input_labels = np.array([1])\n",
    "        else:\n",
    "            # Use center as fallback\n",
    "            h, w = first_frame.shape[:2]\n",
    "            input_points = np.array([[w//2, h//2]])\n",
    "            input_labels = np.array([1])\n",
    "        \n",
    "        # Add points to first frame\n",
    "        frame_idx, object_ids, masks = video_predictor.add_new_points(\n",
    "            inference_state=inference_state,\n",
    "            frame_idx=0,\n",
    "            obj_id=1,\n",
    "            points=input_points,\n",
    "            labels=input_labels,\n",
    "        )\n",
    "        \n",
    "        # Propagate masks through video\n",
    "        video_segments = {}\n",
    "        for out_frame_idx, out_obj_ids, out_mask_logits in video_predictor.propagate_in_video(inference_state):\n",
    "            video_segments[out_frame_idx] = {\n",
    "                out_obj_ids[0]: (out_mask_logits[0] > 0.0).cpu().numpy()\n",
    "            }\n",
    "        \n",
    "        return frames, video_segments\n",
    "    \n",
    "    def visualize_video_segmentation(frames, video_segments, text_prompt, max_frames=6):\n",
    "        \"\"\"Visualize video segmentation results.\"\"\"\n",
    "        n_frames = min(len(frames), max_frames)\n",
    "        \n",
    "        plt.figure(figsize=(20, 8))\n",
    "        \n",
    "        for i in range(n_frames):\n",
    "            frame = frames[i]\n",
    "            mask = video_segments.get(i, {}).get(1, np.zeros(frame.shape[:2], dtype=bool))\n",
    "            \n",
    "            # Create overlay\n",
    "            overlay = frame.copy()\n",
    "            if mask.any():\n",
    "                overlay[mask] = overlay[mask] * 0.6 + np.array([255, 0, 0]) * 0.4\n",
    "            \n",
    "            plt.subplot(2, n_frames//2, i+1)\n",
    "            plt.imshow(overlay.astype(np.uint8))\n",
    "            plt.title(f'Frame {i}: \"{text_prompt}\"')\n",
    "            plt.axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    print(\"Video segmentation functions loaded successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Video segmentation not available: {e}\")\n",
    "    print(\"This requires SAM 2 video predictor which may not be fully available in this environment.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9186acf0",
   "metadata": {},
   "source": [
    "## Video Segmentation Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42214250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example video segmentation (if available)\n",
    "try:\n",
    "    print(\"Upload a short video file (MP4 format, <30 seconds):\")\n",
    "    uploaded_video = files.upload()\n",
    "    \n",
    "    video_filename = list(uploaded_video.keys())[0]\n",
    "    video_text_prompt = input(\"Enter text prompt for video segmentation: \")\n",
    "    \n",
    "    # Run video segmentation\n",
    "    frames, video_segments = video_segmentation_pipeline(video_filename, video_text_prompt)\n",
    "    \n",
    "    # Visualize results\n",
    "    visualize_video_segmentation(frames, video_segments, video_text_prompt)\n",
    "    \n",
    "    # Clean up\n",
    "    os.remove(video_filename)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Video segmentation example skipped: {e}\")\n",
    "    print(\"This feature requires video upload and may not work in all environments.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d021a9",
   "metadata": {},
   "source": [
    "## Pipeline Summary and Analysis\n",
    "\n",
    "### Pipeline Description:\n",
    "1. **Text Input**: User provides a text description of the object to segment\n",
    "2. **Text-to-Region**: Convert text to potential object locations using:\n",
    "   - Primary: GroundingDINO (if available)\n",
    "   - Fallback: CLIP with sliding window approach\n",
    "3. **Region-to-Segmentation**: Use SAM 2 to generate precise segmentation masks\n",
    "4. **Visualization**: Display original image, mask, and overlay\n",
    "\n",
    "### Technical Implementation:\n",
    "- **GroundingDINO**: State-of-the-art open-vocabulary object detection\n",
    "- **CLIP Fallback**: Sliding window approach with vision-language similarity\n",
    "- **SAM 2**: Latest Segment Anything model for high-quality segmentation\n",
    "- **Robust Error Handling**: Multiple fallback mechanisms for reliability\n",
    "\n",
    "### Limitations:\n",
    "1. **Text Ambiguity**: Simple text prompts may not capture complex spatial relationships\n",
    "2. **Single Object Focus**: Currently optimized for single primary object per prompt\n",
    "3. **Computational Requirements**: Requires GPU for optimal performance\n",
    "4. **Model Dependencies**: Relies on large pre-trained models (>1GB each)\n",
    "5. **Video Processing**: Video segmentation requires significant computational resources\n",
    "\n",
    "### Potential Improvements:\n",
    "1. **Multi-object Support**: Handle multiple objects in single prompt\n",
    "2. **Spatial Relationships**: \"dog to the left of the tree\"\n",
    "3. **Interactive Refinement**: Allow user to refine segmentation with additional clicks\n",
    "4. **Real-time Processing**: Optimize for live video segmentation\n",
    "5. **Custom Training**: Fine-tune on domain-specific data"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
