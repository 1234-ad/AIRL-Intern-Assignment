{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54a0e85b",
   "metadata": {},
   "source": [
    "# Q1: Vision Transformer on CIFAR-10\n",
    "\n",
    "Implementation of Vision Transformer (ViT) for CIFAR-10 classification based on \"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\" (Dosovitskiy et al., ICLR 2021).\n",
    "\n",
    "## Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf5f166",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision matplotlib numpy tqdm\n",
    "!pip install timm  # For comparison and some utilities\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import random\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce2deaf",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beff5e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation and normalization\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize to standard ViT input size\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    transforms.RandomErasing(p=0.1)  # Additional augmentation\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=train_transform)\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=test_transform)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "print(f'Training samples: {len(train_dataset)}')\n",
    "print(f'Test samples: {len(test_dataset)}')\n",
    "print(f'Number of classes: {len(train_dataset.classes)}')\n",
    "print(f'Classes: {train_dataset.classes}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380fa19a",
   "metadata": {},
   "source": [
    "## Vision Transformer Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e444235",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    \"\"\"Split image into patches and embed them.\"\"\"\n",
    "    def __init__(self, img_size=224, patch_size=16, in_channels=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.n_patches = (img_size // patch_size) ** 2\n",
    "        \n",
    "        # Use conv2d to extract patches and embed them\n",
    "        self.projection = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, channels, height, width)\n",
    "        x = self.projection(x)  # (batch_size, embed_dim, n_patches_h, n_patches_w)\n",
    "        x = x.flatten(2).transpose(1, 2)  # (batch_size, n_patches, embed_dim)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e069cdd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    \"\"\"Multi-Head Self-Attention mechanism.\"\"\"\n",
    "    def __init__(self, embed_dim=768, num_heads=12, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        \n",
    "        assert embed_dim % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n",
    "        \n",
    "        self.qkv = nn.Linear(embed_dim, embed_dim * 3)\n",
    "        self.projection = nn.Linear(embed_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, embed_dim = x.shape\n",
    "        \n",
    "        # Generate Q, K, V\n",
    "        qkv = self.qkv(x)  # (batch_size, seq_len, embed_dim * 3)\n",
    "        qkv = qkv.reshape(batch_size, seq_len, 3, self.num_heads, self.head_dim)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)  # (3, batch_size, num_heads, seq_len, head_dim)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        \n",
    "        # Attention scores\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        out = torch.matmul(attention_weights, v)\n",
    "        out = out.transpose(1, 2).reshape(batch_size, seq_len, embed_dim)\n",
    "        \n",
    "        # Final projection\n",
    "        out = self.projection(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7780ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"Multi-Layer Perceptron block.\"\"\"\n",
    "    def __init__(self, embed_dim=768, mlp_dim=3072, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(embed_dim, mlp_dim)\n",
    "        self.fc2 = nn.Linear(mlp_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.gelu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d09b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"Transformer encoder block with MHSA + MLP + residual connections + layer norm.\"\"\"\n",
    "    def __init__(self, embed_dim=768, num_heads=12, mlp_dim=3072, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.attention = MultiHeadSelfAttention(embed_dim, num_heads, dropout)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.mlp = MLP(embed_dim, mlp_dim, dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Pre-norm architecture\n",
    "        x = x + self.attention(self.norm1(x))\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad6fb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    \"\"\"Vision Transformer implementation.\"\"\"\n",
    "    def __init__(self, img_size=224, patch_size=16, in_channels=3, num_classes=10, \n",
    "                 embed_dim=768, num_layers=12, num_heads=12, mlp_dim=3072, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Patch embedding\n",
    "        self.patch_embedding = PatchEmbedding(img_size, patch_size, in_channels, embed_dim)\n",
    "        \n",
    "        # CLS token and positional embedding\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, self.patch_embedding.n_patches + 1, embed_dim))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Transformer blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(embed_dim, num_heads, mlp_dim, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Classification head\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.head = nn.Linear(embed_dim, num_classes)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            if m.bias is not None:\n",
    "                nn.init.normal_(m.bias, std=1e-6)\n",
    "        elif isinstance(m, nn.Conv2d):\n",
    "            nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        # Patch embedding\n",
    "        x = self.patch_embedding(x)  # (batch_size, n_patches, embed_dim)\n",
    "        \n",
    "        # Add CLS token\n",
    "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
    "        x = torch.cat([cls_tokens, x], dim=1)  # (batch_size, n_patches + 1, embed_dim)\n",
    "        \n",
    "        # Add positional embedding\n",
    "        x = x + self.pos_embedding\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Pass through transformer blocks\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "            \n",
    "        # Classification from CLS token\n",
    "        x = self.norm(x)\n",
    "        cls_token_final = x[:, 0]  # Take CLS token\n",
    "        \n",
    "        return self.head(cls_token_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737ccc44",
   "metadata": {},
   "source": [
    "## Model Configuration and Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0331adcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration - optimized for CIFAR-10\n",
    "config = {\n",
    "    'img_size': 224,\n",
    "    'patch_size': 16,\n",
    "    'in_channels': 3,\n",
    "    'num_classes': 10,\n",
    "    'embed_dim': 384,  # Smaller than standard ViT for efficiency\n",
    "    'num_layers': 6,   # Fewer layers for CIFAR-10\n",
    "    'num_heads': 6,\n",
    "    'mlp_dim': 1536,   # 4 * embed_dim\n",
    "    'dropout': 0.1\n",
    "}\n",
    "\n",
    "# Create model\n",
    "model = VisionTransformer(**config).to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'Total parameters: {total_params:,}')\n",
    "print(f'Trainable parameters: {trainable_params:,}')\n",
    "\n",
    "# Training configuration\n",
    "num_epochs = 100\n",
    "learning_rate = 3e-4\n",
    "weight_decay = 0.1\n",
    "\n",
    "# Optimizer and scheduler\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=1e-6)\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "\n",
    "print(f'\\nTraining configuration:')\n",
    "print(f'Epochs: {num_epochs}')\n",
    "print(f'Learning rate: {learning_rate}')\n",
    "print(f'Weight decay: {weight_decay}')\n",
    "print(f'Batch size: {batch_size}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e904150",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55112f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc='Training')\n",
    "    for batch_idx, (data, target) in enumerate(pbar):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = output.max(1)\n",
    "        total += target.size(0)\n",
    "        correct += predicted.eq(target).sum().item()\n",
    "        \n",
    "        pbar.set_postfix({\n",
    "            'Loss': f'{running_loss/(batch_idx+1):.4f}',\n",
    "            'Acc': f'{100.*correct/total:.2f}%'\n",
    "        })\n",
    "    \n",
    "    return running_loss/len(train_loader), 100.*correct/total\n",
    "\n",
    "def evaluate(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(test_loader, desc='Evaluating')\n",
    "        for data, target in pbar:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item()\n",
    "            \n",
    "            _, predicted = output.max(1)\n",
    "            total += target.size(0)\n",
    "            correct += predicted.eq(target).sum().item()\n",
    "            \n",
    "            pbar.set_postfix({\n",
    "                'Acc': f'{100.*correct/total:.2f}%'\n",
    "            })\n",
    "    \n",
    "    return test_loss/len(test_loader), 100.*correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1e9a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "test_losses = []\n",
    "test_accuracies = []\n",
    "best_test_acc = 0.0\n",
    "\n",
    "print('Starting training...')\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'\\nEpoch {epoch+1}/{num_epochs}')\n",
    "    print(f'Learning rate: {optimizer.param_groups[0][\"lr\"]:.6f}')\n",
    "    \n",
    "    # Train\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "    \n",
    "    # Evaluate\n",
    "    test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
    "    \n",
    "    # Update scheduler\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Save metrics\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "    test_losses.append(test_loss)\n",
    "    test_accuracies.append(test_acc)\n",
    "    \n",
    "    # Save best model\n",
    "    if test_acc > best_test_acc:\n",
    "        best_test_acc = test_acc\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'test_acc': test_acc,\n",
    "            'config': config\n",
    "        }, 'best_vit_cifar10.pth')\n",
    "        print(f'New best test accuracy: {best_test_acc:.2f}%')\n",
    "    \n",
    "    print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')\n",
    "    print(f'Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%')\n",
    "\n",
    "print(f'\\nTraining completed!')\n",
    "print(f'Best test accuracy: {best_test_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d76e76",
   "metadata": {},
   "source": [
    "## Results Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b088cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(test_losses, label='Test Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Test Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(train_accuracies, label='Train Accuracy')\n",
    "plt.plot(test_accuracies, label='Test Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Training and Test Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot([optimizer.param_groups[0]['lr'] * (0.5 ** (epoch // 30)) for epoch in range(len(test_accuracies))])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.title('Learning Rate Schedule')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f'Final Results:')\n",
    "print(f'Best Test Accuracy: {best_test_acc:.2f}%')\n",
    "print(f'Final Train Accuracy: {train_accuracies[-1]:.2f}%')\n",
    "print(f'Final Test Accuracy: {test_accuracies[-1]:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0716ce",
   "metadata": {},
   "source": [
    "## Model Analysis and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be25317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model for final evaluation\n",
    "checkpoint = torch.load('best_vit_cifar10.pth')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# Final evaluation\n",
    "final_test_loss, final_test_acc = evaluate(model, test_loader, criterion, device)\n",
    "print(f'Final Test Accuracy with Best Model: {final_test_acc:.2f}%')\n",
    "\n",
    "# Per-class accuracy\n",
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "classes = train_dataset.classes\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        outputs = model(data)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        c = (predicted == target).squeeze()\n",
    "        for i in range(target.size(0)):\n",
    "            label = target[i]\n",
    "            class_correct[label] += c[i].item()\n",
    "            class_total[label] += 1\n",
    "\n",
    "print('\\nPer-class accuracy:')\n",
    "for i in range(10):\n",
    "    print(f'{classes[i]}: {100 * class_correct[i] / class_total[i]:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e913bc2",
   "metadata": {},
   "source": [
    "## Sample Predictions Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49becf5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some predictions\n",
    "def visualize_predictions(model, test_loader, classes, num_images=8):\n",
    "    model.eval()\n",
    "    images_shown = 0\n",
    "    \n",
    "    plt.figure(figsize=(15, 8))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            outputs = model(data)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            for i in range(data.size(0)):\n",
    "                if images_shown >= num_images:\n",
    "                    break\n",
    "                    \n",
    "                # Denormalize image for display\n",
    "                img = data[i].cpu()\n",
    "                img = img * torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1) + torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "                img = torch.clamp(img, 0, 1)\n",
    "                \n",
    "                plt.subplot(2, 4, images_shown + 1)\n",
    "                plt.imshow(img.permute(1, 2, 0))\n",
    "                plt.title(f'True: {classes[target[i]]}, Pred: {classes[predicted[i]]}')\n",
    "                plt.axis('off')\n",
    "                \n",
    "                images_shown += 1\n",
    "            \n",
    "            if images_shown >= num_images:\n",
    "                break\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_predictions(model, test_loader, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09dc3a4",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This implementation includes:\n",
    "1. **Patch Embedding**: Images are split into 16x16 patches and linearly embedded\n",
    "2. **Positional Embeddings**: Learnable positional embeddings added to patch embeddings\n",
    "3. **CLS Token**: Special classification token prepended to sequence\n",
    "4. **Transformer Blocks**: Multi-head self-attention + MLP with residual connections and layer normalization\n",
    "5. **Classification Head**: Final prediction from CLS token\n",
    "\n",
    "**Optimizations for CIFAR-10**:\n",
    "- Smaller model (384 embed_dim, 6 layers) for efficiency\n",
    "- Strong data augmentation (rotation, color jitter, random erasing)\n",
    "- Label smoothing and gradient clipping\n",
    "- Cosine annealing learning rate schedule\n",
    "- AdamW optimizer with weight decay"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
